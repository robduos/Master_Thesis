{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4a4e25b-8b7e-4ee1-9f29-0a2a7de443af",
   "metadata": {},
   "source": [
    "Script Description: Merge all the individual datasets to one pre-processed dataset\n",
    "\n",
    "File Name: 01_09_Final_Merging.ipynb\n",
    "\n",
    "Date: 2025\n",
    "\n",
    "Created by: Rob Alamgir\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "References:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2cdc6-1e44-4880-b2f6-db2060e9152f",
   "metadata": {},
   "source": [
    "#### Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e14cd30d-3392-4d28-8d1e-1ea75e392e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb55eae-66c7-4108-aef9-b7b207cef7e6",
   "metadata": {},
   "source": [
    "#### Import all the relevant datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20679a57-f67b-4366-a5e7-6abc44c83035",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"C:/Data_MSc_Thesis/\"\n",
    "\n",
    "# Get file paths of Remotely Sensed Data\n",
    "Planet_SWC_Data = pd.read_csv(\"C:/Data_MSc_Thesis/Planet/Planet_SWC_Extracted.csv\") \n",
    "\n",
    "S1_SAR_VSM_CSV_Files = glob.glob(os.path.join(base_dir, \"S1_SAR_VSM/Final_S1_SAR_VSM_Data\", \"*VSM.csv\"))\n",
    "S1_Backscatter_CSV_Files = glob.glob(os.path.join(base_dir, \"S1_SAR_Backscatter\", \"*Scat.csv\"))\n",
    "S2_Indices_CSV_Files = glob.glob(os.path.join(base_dir, \"S2_Indices\", \"*.csv\"))\n",
    "L8_9_LST_CSV_Files = glob.glob(os.path.join(base_dir, \"L8_L9_LST\", \"*.csv\"))\n",
    "MODIS_CSV_Files = glob.glob(os.path.join(base_dir, \"MODIS_LAI\", \"*.csv\"))\n",
    "\n",
    "# Get file paths of Hybrid Sensed Data\n",
    "OWASIS_Data = pd.read_csv(\"C:/Data_MSc_Thesis/OWASIS/OWASIS_NOBV_extracted_data.csv\")\n",
    "BIS_4D_Data = pd.read_csv(\"C:/Data_MSc_Thesis/BIS_4D_Selected/NOBV_Point_Data_Extracted_V1.csv\") \n",
    "BOFEK_Data = pd.read_csv(\"C:/Data_MSc_Thesis/BOFEK/BOFEK_NOBV.csv\")                             \n",
    "\n",
    "# Get file paths of Ground Sensed Data\n",
    "EC_Tower_Data = pd.read_csv(\"C:/Data_MSc_Thesis/Pre_Processed_Data_Final/Pre_Processed_Data_All_Locations_V5.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb9d969d-199e-4b7d-9050-aa2d1e0d7807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read a file and add a Source column\n",
    "def read_and_add_name(file):\n",
    "    base_name = os.path.splitext(os.path.basename(file))[0]\n",
    "    df = pd.read_csv(file)\n",
    "    df['Source'] = base_name\n",
    "    return df\n",
    "\n",
    "# Load and process S1_SAR_VSM data\n",
    "S1_SAR_VSM_merged = pd.concat([read_and_add_name(file) for file in S1_SAR_VSM_CSV_Files], ignore_index=True)\n",
    "S1_SAR_VSM_merged.rename(columns={\"system:time_start\": \"Date\", \"VSM\":\"S1_VSM\", \"Source\":\"Site_ID\"}, inplace=True)\n",
    "S1_SAR_VSM_merged.drop(columns=[\"pixel_count\"], inplace=True)\n",
    "S1_SAR_VSM_merged['Date'] = pd.to_datetime(S1_SAR_VSM_merged['Date'], errors='coerce')\n",
    "S1_SAR_VSM_merged = S1_SAR_VSM_merged[S1_SAR_VSM_merged[\"Date\"] >= \"2020-01-01\"]\n",
    "S1_SAR_VSM_merged['Site_ID'] = S1_SAR_VSM_merged['Site_ID'].str.replace(r'^S1_SAR_|_VSM$', '', regex=True)\n",
    "\n",
    "# Load and process S1_Backscatter data\n",
    "S1_Backscatter_merged = pd.concat([read_and_add_name(file) for file in S1_Backscatter_CSV_Files], ignore_index=True)\n",
    "S1_Backscatter_merged.rename(columns={\"mean\": \"S1_Backscatter\", \"Source\":\"Site_ID\"}, inplace=True)\n",
    "S1_Backscatter_merged.drop(columns=[\"count\"], inplace=True)\n",
    "S1_Backscatter_merged['Date'] = pd.to_datetime(S1_Backscatter_merged['Date'], errors='coerce')\n",
    "S1_Backscatter_merged = S1_Backscatter_merged[S1_Backscatter_merged[\"Date\"] >= \"2020-01-01\"]\n",
    "S1_Backscatter_merged['Date'] = S1_Backscatter_merged['Date'].dt.date\n",
    "S1_Backscatter_merged['Date'] = pd.to_datetime(S1_Backscatter_merged['Date'], errors='coerce')\n",
    "S1_Backscatter_merged = S1_Backscatter_merged[S1_Backscatter_merged[\"Site_ID\"] != \"S1_SAR_WRW_OW_Back_Scat\"]\n",
    "S1_Backscatter_merged['Site_ID'] = S1_Backscatter_merged['Site_ID'].str.replace(r'^S1_SAR_|_Back_Scat$', '', regex=True)\n",
    "\n",
    "# Read and merge S2_Indices Data\n",
    "S2_Indices_merged = pd.concat([read_and_add_name(file) for file in S2_Indices_CSV_Files], ignore_index=True)\n",
    "S2_Indices_merged = S2_Indices_merged.rename(columns={'NDVI': 'S2_NDVI', 'EVI': 'S2_EVI', 'NDMI': 'S2_NDMI', 'Source':'Site_ID'})\n",
    "S2_Indices_merged.drop(columns=['MNDWI', 'STR'], errors='ignore', inplace=True)\n",
    "S2_Indices_merged['Date'] = pd.to_datetime(S2_Indices_merged['Date'], errors='coerce')\n",
    "S2_Indices_merged.dropna(inplace=True)\n",
    "S2_Indices_merged = S2_Indices_merged[S2_Indices_merged[\"Date\"] >= \"2020-01-01\"]\n",
    "S2_Indices_merged['Site_ID'] = S2_Indices_merged['Site_ID'].str.replace(r'^S2_Indices_|$', '', regex=True)\n",
    "\n",
    "# Read and merge L8_9_LST Data\n",
    "L8_9_LST_merged = pd.concat([read_and_add_name(file) for file in L8_9_LST_CSV_Files], ignore_index=True)\n",
    "L8_9_LST_merged = L8_9_LST_merged.rename(columns={'Mean_Surface_Temperature': 'L8_9_LST', 'Source':'Site_ID'})\n",
    "L8_9_LST_merged['Date'] = pd.to_datetime(L8_9_LST_merged['Date'], errors='coerce')\n",
    "L8_9_LST_merged.drop(columns=['system:index', '.geo'], errors='ignore', inplace=True)\n",
    "L8_9_LST_merged = L8_9_LST_merged.loc[:, ~L8_9_LST_merged.columns.duplicated()]\n",
    "L8_9_LST_merged = L8_9_LST_merged[L8_9_LST_merged[\"Date\"] >= \"2020-01-01\"]\n",
    "L8_9_LST_merged['Site_ID'] = L8_9_LST_merged['Site_ID'].str.replace(r'^L8_9_LST_|$', '', regex=True)\n",
    "\n",
    "# Read and merge MODIS LAI Data\n",
    "MODIS_LAI_merged = pd.concat([read_and_add_name(file) for file in MODIS_CSV_Files], ignore_index=True)\n",
    "MODIS_LAI_merged = MODIS_LAI_merged.rename(columns={'Mean_LAI': 'MODIS_LAI', 'Source':'Site_ID'})\n",
    "MODIS_LAI_merged['Date'] = pd.to_datetime(MODIS_LAI_merged['Date'], errors='coerce')\n",
    "MODIS_LAI_merged.drop(columns=['system:index', '.geo', 'Unnamed: 0'], errors='ignore', inplace=True)\n",
    "MODIS_LAI_merged = MODIS_LAI_merged.loc[:, ~MODIS_LAI_merged.columns.duplicated()]\n",
    "MODIS_LAI_merged = MODIS_LAI_merged[MODIS_LAI_merged[\"Date\"] >= \"2020-01-01\"]\n",
    "MODIS_LAI_merged['Site_ID'] = MODIS_LAI_merged['Site_ID'].str.replace(r'^MODIS_LAI_|$', '', regex=True)\n",
    "\n",
    "# Pre-process Planet_SWC_Data\n",
    "Planet_SWC_Data.rename(columns={'timestamp': 'Date','swc':'Planet_SWC'}, inplace=True)\n",
    "Planet_SWC_Data['Date'] = pd.to_datetime(Planet_SWC_Data['Date']).dt.normalize()\n",
    "Planet_SWC_Data.drop(columns=['Matched_X', 'Matched_Y', 'swc-qf','Source'], errors='ignore', inplace=True)\n",
    "\n",
    "# Pre-process OWASIS_Data\n",
    "OWASIS_Data.rename(columns={'Datetime': 'Date'}, inplace=True)\n",
    "OWASIS_Data['Date'] = pd.to_datetime(OWASIS_Data['Date'], errors='coerce')\n",
    "\n",
    "# Pre-process BIS_4D_Data\n",
    "BIS_4D_Data.drop(columns=['Site_no', 'Location_No', 'Longitude', 'Latitude','EPSG_32631_WGS.84_X_m',\n",
    "                          'EPSG_32631_WGS.84_Y_m','Reproj_X','Reproj_Y','Elevation_m','geometry'], errors='ignore', inplace=True)\n",
    "# Pre-process BOFEK_Data\n",
    "BOFEK_Data.drop(columns=['Site_Name', 'BOFEK_2020_PU_Description'], errors='ignore', inplace=True)\n",
    "\n",
    "# Pre-process EC Tower Data\n",
    "EC_Tower_Data['Date'] = pd.to_datetime(EC_Tower_Data['Date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e6112-f2f4-44d1-87f8-fc716c453b14",
   "metadata": {},
   "source": [
    "#### Merge all the individual datasets to one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d8653a6-838f-4962-9806-371e57420dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with 'Date' column\n",
    "time_dependent_dfs = [\n",
    "    S1_SAR_VSM_merged,\n",
    "    S1_Backscatter_merged,\n",
    "    Planet_SWC_Data,\n",
    "    OWASIS_Data,\n",
    "    S2_Indices_merged,\n",
    "    L8_9_LST_merged,\n",
    "    MODIS_LAI_merged,\n",
    "    EC_Tower_Data\n",
    "]\n",
    "\n",
    "# Merge all time-dependent datasets on 'Date' and 'Site_ID'\n",
    "merged_df = time_dependent_dfs[0]\n",
    "for df in time_dependent_dfs[1:]:\n",
    "    merged_df = merged_df.merge(df, on=['Date', 'Site_ID'], how='outer')\n",
    "\n",
    "# Merge non-time-dependent datasets based on 'Site_ID'\n",
    "merged_df = merged_df.merge(BIS_4D_Data, on='Site_ID', how='left')\n",
    "merged_df = merged_df.merge(BOFEK_Data, on='Site_ID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b523a66e-1180-4193-b27c-05f6d8cd53bd",
   "metadata": {},
   "source": [
    "#### Re-order the columns of the merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edd3bedb-df06-4f65-b670-408794fd497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new order for the first columns\n",
    "first_columns = ['Date', 'year_month', 'DOY', 'Site_ID']\n",
    "\n",
    "# Get the remaining columns (excluding the ones already selected)\n",
    "remaining_columns = [col for col in merged_df.columns if col not in first_columns]\n",
    "\n",
    "# Reorder the dataframe\n",
    "merged_df = merged_df[first_columns + remaining_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf67baa9-3015-4060-9849-9a0446929119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of columns\n",
    "cols = merged_df.columns.to_list()\n",
    "\n",
    "# Move 'DOY' after 'Date' and 'Site_ID' after 'DOY'\n",
    "cols.remove('DOY')\n",
    "cols.remove('Site_ID')\n",
    "cols.insert(1, 'DOY')  # Insert 'DOY' at index 1 (after 'Date')\n",
    "cols.insert(2, 'Site_ID')  # Insert 'Site_ID' at index 2 (after 'DOY')\n",
    "\n",
    "# Reorder the dataframe\n",
    "merged_df = merged_df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f765bf74-3f1c-4cf2-9018-10431c694d86",
   "metadata": {},
   "source": [
    "#### Further dataframe post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "961a2d3c-4037-472f-99bf-4ac9f7659ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[merged_df[\"Site_ID\"] != \"ARM\"].reset_index(drop=True)\n",
    "merged_df.drop(columns=[\"EPSG_32631_WGS 84_X_m\", \"EPSG_32631_WGS 84_Y_m\", \"P_minus_ET.1\", \"P_minus_PET.1\"], inplace=True)\n",
    "\n",
    "columns_to_divide = [\n",
    "    \"SWCT_1_005\", \"SWCT_1_015\", \"SWCT_1_025\", \"SWCT_1_035\", \"SWCT_1_045\",\n",
    "    \"SWCT_1_055\", \"SWCT_1_065\", \"SWCT_1_075\", \"SWCT_1_085\", \"SWCT_1_095\",\n",
    "    \"SWCT_1_105\", \"SWCT_1_115\"]\n",
    "\n",
    "merged_df[columns_to_divide] = merged_df[columns_to_divide] / 100\n",
    "#print(merged_df[columns_to_divide].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85d3c7b5-0502-412b-bf4f-964b7313ba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "DOY\n",
      "Site_ID\n",
      "year_month\n",
      "S1_VSM\n",
      "S1_Backscatter\n",
      "Planet_SWC\n",
      "RZSM_OWASIS\n",
      "GWL_OWASIS\n",
      "AWS_OWASIS\n",
      "S2_NDVI\n",
      "S2_EVI\n",
      "S2_NDMI\n",
      "L8_9_LST\n",
      "MODIS_LAI\n",
      "year_week\n",
      "SWCT_1_005\n",
      "SWCT_1_015\n",
      "SWCT_1_025\n",
      "SWCT_1_035\n",
      "SWCT_1_045\n",
      "SWCT_1_055\n",
      "SWCT_1_065\n",
      "SWCT_1_075\n",
      "SWCT_1_085\n",
      "SWCT_1_095\n",
      "SWCT_1_105\n",
      "SWCT_1_115\n",
      "STMP_1_005\n",
      "STMP_1_015\n",
      "STMP_1_025\n",
      "STMP_1_035\n",
      "STMP_1_045\n",
      "STMP_1_055\n",
      "STMP_1_065\n",
      "STMP_1_075\n",
      "STMP_1_085\n",
      "STMP_1_095\n",
      "STMP_1_105\n",
      "STMP_1_115\n",
      "WLEV_f\n",
      "WTMP_f\n",
      "ATMP_f\n",
      "PAIR_f\n",
      "WIND_f\n",
      "WINS_f\n",
      "RHUM_f\n",
      "RAIN_f\n",
      "RAIN_f_monthly_sum\n",
      "RAIN_f_weekly_sum\n",
      "VPD_f\n",
      "SWIN_f\n",
      "ET\n",
      "PET\n",
      "ET0\n",
      "P_minus_ET\n",
      "P_minus_PET\n",
      "NEE_CO2\n",
      "NEE_CH4\n",
      "GPP_NT\n",
      "RECO_NT\n",
      "NEE_CO2_kg_day_ha_DAv_24hrs\n",
      "NEE_CH4_kg_day_ha_DAv_24hrs\n",
      "GPP_NT_kg_day_ha_DAv_24hrs\n",
      "RECO_NT_kg_day_ha_DAv_24hrs\n",
      "NEE_CO2_kg_day_ha_DAv_NT\n",
      "NEE_CH4_kg_day_ha_DAv_NT\n",
      "GPP_NT_kg_day_ha_DAv_NT\n",
      "RECO_NT_kg_day_ha_DAv_NT\n",
      "NEE_CO2_kg_day_ha_DAv_DT\n",
      "NEE_CH4_kg_day_ha_DAv_DT\n",
      "GPP_NT_kg_day_ha_DAv_DT\n",
      "RECO_NT_kg_day_ha_DAv_DT\n",
      "GPP_NT_kg_day_ha_DAv_NaN\n",
      "RECO_NT_kg_day_ha_DAv_NaN\n",
      "NEE_CO2_kg_day_ha_WAv_24hrs\n",
      "NEE_CH4_kg_day_ha_WAv_24hrs\n",
      "GPP_NT_kg_day_ha_WAv_24hrs\n",
      "RECO_NT_kg_day_ha_WAv_24hrs\n",
      "NEE_CO2_kg_day_ha_WAv_NT\n",
      "NEE_CH4_kg_day_ha_WAv_NT\n",
      "GPP_NT_kg_day_ha_WAv_NT\n",
      "RECO_NT_kg_day_ha_WAv_NT\n",
      "NEE_CO2_kg_day_ha_WAv_DT\n",
      "NEE_CH4_kg_day_ha_WAv_DT\n",
      "GPP_NT_kg_day_ha_WAv_DT\n",
      "RECO_NT_kg_day_ha_WAv_DT\n",
      "GPP_NT_kg_day_ha_WAv_NaN\n",
      "RECO_NT_kg_day_ha_WAv_NaN\n",
      "BD_0_5\n",
      "BD_5_15\n",
      "Clay_0_5\n",
      "Clay_5_15\n",
      "Sand_0_5\n",
      "Sand_5_15\n",
      "Silt_0_5\n",
      "Silt_5_15\n",
      "SOM_2020_0_5\n",
      "SOM_2020_5_15\n",
      "SOM_2023_0_5\n",
      "SOM_2023_5_15\n",
      "BOFEK_2020_Physical Units\n",
      "Peat_Thickness_2022\n"
     ]
    }
   ],
   "source": [
    "# Check missing values\n",
    "#print(merged_df.info())\n",
    "for col in merged_df.columns:\n",
    "    print(col)\n",
    "#print(merged_df.head(30))\n",
    "#print(merged_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec6af7-c7d1-4c9d-99b8-8ce6c9153cb2",
   "metadata": {},
   "source": [
    "#### Export the final dataframe to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2df0c5c-b73f-44f0-94ce-e2e888ed222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully saved to C:/Data_MSc_Thesis/Pre_Processed_Data_Final/Pre_Processed_Data_All_Locations_V6.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = \"C:/Data_MSc_Thesis/Pre_Processed_Data_Final/Pre_Processed_Data_All_Locations_V6.csv\"  # Update the path as needed\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"DataFrame successfully saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
