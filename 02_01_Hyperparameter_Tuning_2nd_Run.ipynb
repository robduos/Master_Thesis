{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4645ed2-0f7b-415e-88bf-39dc82f05dd3",
   "metadata": {},
   "source": [
    "Script Description\n",
    "This script loads a pre-processed dataset, prepares features and target variables for predicting soil water content (SWCT_015), and optimizes a Extreme Gradient Boosting regression model using grouped cross-validation.\n",
    "\n",
    "File Name: 02_01_Hyperparameter_Tuning.ipynb\n",
    "\n",
    "Date: 2025\n",
    "\n",
    "Created by: Rob Alamgir\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "References:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ff005c-fdf8-47ef-8bfa-d443f2a6f956",
   "metadata": {},
   "source": [
    "#### Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92aa91a-0be1-441e-b0ea-d0854e56e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, LeaveOneGroupOut, ParameterGrid\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.cm as cm\n",
    "#help(XGBRegressor)\n",
    "#help(RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aaae67-7f3a-4d17-9dfb-b3f86bff4e24",
   "metadata": {},
   "source": [
    "### Step 1: Load Data & prep the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ef73e2-4ef2-43c0-ac3d-bd9124cbc3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows and columns before removing NaNs: (36222, 109)\n",
      "Rows and columns after removing NaNs: (8576, 109)\n",
      "Features (X): (8576, 13), Target (y): (8576,), Groups: (8576,), Date: (8576,)\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "data_path = \"C:/Data_MSc_Thesis/Pre_Processed_Data/Pre_Processed_Data_All_Locations_Updated_6.csv\"\n",
    "#data_path = \"C:/Data_MSc_Thesis/Pre_Processed_Data/Pre_Processed_Data_All_Locations_Updated_6_Summer_Data.csv\"\n",
    "#data_path = \"C:/Data_MSc_Thesis/Pre_Processed_Data/Pre_Processed_Data_All_Locations_Updated_6_Winter_Data.csv\"\n",
    "Complete_Data = pd.read_csv(data_path)\n",
    "\n",
    "# Data preprocessing\n",
    "Complete_Data['Date'] = pd.to_datetime(Complete_Data['Date'], format='%Y-%m-%d')\n",
    "Complete_Data['Source_ID'] = Complete_Data['Source'].astype('category').cat.codes + 1\n",
    "print(f\"Rows and columns before removing NaNs: {Complete_Data.shape}\")\n",
    "\n",
    "# Filter and clean data\n",
    "filtered_df = Complete_Data.dropna(subset=['SWCT_1_015']).copy() \n",
    "filtered_df['BOFEK_2020_Physical Units'] = filtered_df['BOFEK_2020_Physical Units'].astype('category')\n",
    "print(f\"Rows and columns after removing NaNs: {filtered_df.shape}\")\n",
    "\n",
    "# Feature and target selection\n",
    "RS_GSD_Features = ['S1_Backscatter', 'S2_NDVI', 'S2_EVI', 'S2_NDMI', \n",
    "                   'STMP_1_015', 'ATMP_f', 'WTMP_f', 'WLEV_f', 'VPD_f',\n",
    "                   'NEE_CO2_kg_day_ha_DAv_NT', 'NEE_CH4_kg_day_ha_DAv_NT',\n",
    "                   'BD_0_5_values', 'Peat_Thickness_2022']\n",
    "\n",
    "\n",
    "X = filtered_df[RS_GSD_Features]    # Features   \n",
    "y = filtered_df['SWCT_1_015']       # Predictor\n",
    "groups = filtered_df[\"Source_ID\"]   # Groups for Leave-One-Group-Out\n",
    "dates = filtered_df['Date']\n",
    "\n",
    "print(f\"Features (X): {X.shape}, Target (y): {y.shape}, Groups: {groups.shape}, Date: {dates.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3ee16c-de39-485e-8ebb-3a4b19748d47",
   "metadata": {},
   "source": [
    "### Step 2: Split the dataset to a training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67eeb1b7-23b6-40fc-b88d-9149b009625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes after splitting and alignment:\n",
      "Train set: X_train: (7718, 13), y_train: (7718,), groups_train: (7718,), dates_train: (7718,)\n",
      "Test set: X_test: (858, 13), y_test: (858,), groups_test: (858,), dates_test: (858,)\n"
     ]
    }
   ],
   "source": [
    "# Perform the train-test split with temporal separation\n",
    "X_train, X_test, y_train, y_test, groups_train, groups_test, dates_train, dates_test = train_test_split(\n",
    "    X, y, groups, dates,\n",
    "    test_size=0.1,      # Reserve 10%/30% for the test set\n",
    "    shuffle=False)      # Ensure temporal order is maintained\n",
    "\n",
    "# Ensure alignment of training data\n",
    "X_train, y_train = X_train.align(y_train, join='inner', axis=0)\n",
    "groups_train = groups_train.loc[X_train.index]\n",
    "dates_train = dates_train.loc[X_train.index]  # Align dates_train with X_train\n",
    "\n",
    "# Ensure alignment of test data\n",
    "X_test, y_test = X_test.align(y_test, join='inner', axis=0)\n",
    "groups_test = groups_test.loc[X_test.index]\n",
    "dates_test = dates_test.loc[X_test.index]  # Align dates_test with X_test\n",
    "\n",
    "# Verify alignment\n",
    "assert X_train.index.equals(y_train.index) and X_train.index.equals(groups_train.index) and X_train.index.equals(dates_train.index), \\\n",
    "    \"Rows in X_train, y_train, groups_train, and dates_train are misaligned!\"\n",
    "\n",
    "assert X_test.index.equals(y_test.index) and X_test.index.equals(groups_test.index) and X_test.index.equals(dates_test.index), \\\n",
    "    \"Rows in X_test, y_test, groups_test, and dates_test are misaligned!\"\n",
    "\n",
    "# Print shapes for verification\n",
    "print(\"Data shapes after splitting and alignment:\")\n",
    "print(f\"Train set: X_train: {X_train.shape}, y_train: {y_train.shape}, groups_train: {groups_train.shape}, dates_train: {dates_train.shape}\")\n",
    "print(f\"Test set: X_test: {X_test.shape}, y_test: {y_test.shape}, groups_test: {groups_test.shape}, dates_test: {dates_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847693a3-4481-4e9a-bd84-6ae06fc876c8",
   "metadata": {},
   "source": [
    "### Step 3: Define the model, hyperparameter grid and run the Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16e8338a-a181-4c29-a6ec-a95bb5b58114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress: 100%|████████████████████████████████████████████████████████| 486/486 [1:37:49<00:00, 12.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters: {'xgbregressor__colsample_bytree': 0.5, 'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 7, 'xgbregressor__n_estimators': 800, 'xgbregressor__scale_pos_weight': 1, 'xgbregressor__subsample': 0.9}\n",
      "Best Cross-Validation Score (R²): 0.5101173328404583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBRegressor(random_state=0, enable_categorical=True)  # Define the XGBoost model\n",
    "pipeline = Pipeline([(\"xgbregressor\", xgb)])\n",
    "\n",
    "param_grid = {\"xgbregressor__n_estimators\": [800, 900, 1000],\n",
    "              \"xgbregressor__max_depth\": [5, 6, 7],\n",
    "              \"xgbregressor__learning_rate\": [0.05, 0.1, 0.2],\n",
    "              \"xgbregressor__subsample\": [0.5, 0.7, 0.9],\n",
    "              \"xgbregressor__colsample_bytree\": [0.5, 0.6, 0.7],\n",
    "              \"xgbregressor__scale_pos_weight\": [1, 5]}\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "warnings.simplefilter(\"error\", FitFailedWarning)\n",
    "\n",
    "# Define custom scoring functions\n",
    "def mae_scorer(y_true, y_pred):\n",
    "    return -mean_absolute_error(y_true, y_pred)\n",
    "def mse_scorer(y_true, y_pred):\n",
    "    return -mean_squared_error(y_true, y_pred)\n",
    "def bias_scorer(y_true, y_pred):\n",
    "    return np.mean(y_true - y_pred)\n",
    "\n",
    "# Wrap the custom scorers using make_scorer\n",
    "scoring = {'r2': 'r2',\n",
    "           'mae': make_scorer(mae_scorer),\n",
    "           'mse': make_scorer(mse_scorer),\n",
    "           'bias': make_scorer(bias_scorer)}\n",
    "\n",
    "param_combinations = list(ParameterGrid(param_grid))    # Get the total number of parameter combinations\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=logo,\n",
    "                           scoring=scoring,\n",
    "                           refit='r2',\n",
    "                           n_jobs=-1,\n",
    "                           error_score=0,\n",
    "                           verbose=0)  # Avoid excessive logging\n",
    "\n",
    "# Wrap single values in lists for GridSearchCV\n",
    "with tqdm(total=len(param_combinations), desc=\"Grid Search Progress\") as pbar:\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    results_list = []\n",
    "    \n",
    "    for params in param_combinations:\n",
    "        try:\n",
    "            # Convert params into a properly formatted grid dictionary\n",
    "            formatted_param_grid = {key: [value] for key, value in params.items()}\n",
    "            \n",
    "            grid_search.set_params(param_grid=formatted_param_grid)\n",
    "            grid_search.fit(X_train, y_train, groups=groups_train)\n",
    "\n",
    "            # Capture the best model\n",
    "            if grid_search.best_score_ > best_score:\n",
    "                best_score = grid_search.best_score_\n",
    "                best_params = grid_search.best_params_\n",
    "                best_model = grid_search.best_estimator_\n",
    "            \n",
    "            # Extract and store results\n",
    "            cv_results = grid_search.cv_results_\n",
    "            results_list.append({\n",
    "                'n_estimators': params['xgbregressor__n_estimators'],\n",
    "                'max_depth': params['xgbregressor__max_depth'],\n",
    "                'learning_rate': params['xgbregressor__learning_rate'],\n",
    "                'subsample': params['xgbregressor__subsample'],\n",
    "                'colsample_bytree': params['xgbregressor__colsample_bytree'],\n",
    "                'scale_pos_weight': params['xgbregressor__scale_pos_weight'],\n",
    "                'mean_test_r2': cv_results['mean_test_r2'].mean(),\n",
    "                'mean_test_mae': -cv_results['mean_test_mae'].mean(),\n",
    "                'mean_test_mse': -cv_results['mean_test_mse'].mean(),\n",
    "                'mean_test_bias': cv_results['mean_test_bias'].mean()\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error encountered for params {params}: {e}\")\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results_list)                      # Convert results to a DataFrame\n",
    "print(\"\\nBest Parameters:\", best_params)                     # Print best parameters and score\n",
    "print(\"Best Cross-Validation Score (R²):\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e3a58d-9bf8-4162-b066-05c55c285765",
   "metadata": {},
   "source": [
    "### Step 5: Save the Grid Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a763aa29-69c2-4a48-8995-c886f4be6c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV for reference\n",
    "results_df.to_csv(\"C:/Data_MSc_Thesis/Results/HP_3_grid_search_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
