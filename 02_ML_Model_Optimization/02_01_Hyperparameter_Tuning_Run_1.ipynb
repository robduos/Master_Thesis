{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4645ed2-0f7b-415e-88bf-39dc82f05dd3",
   "metadata": {},
   "source": [
    "**Script Description:** This script loads a pre-processed dataset, prepares features and target variables for predicting SENTEK Soil Moisture Content (SENTEK_SMC), and optimizes a Extreme Gradient Boosting regression model using grouped cross-validation.\n",
    "\n",
    "**File Name:** 02_01_Hyperparameter_Tuning_Run_1.ipynb\n",
    "\n",
    "**Date:** 2025\n",
    "\n",
    "**Created by:** Rob Alamgir "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e2b8a-01b8-4238-b7f7-3268e20bc58b",
   "metadata": {},
   "source": [
    "##### Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42816ab1-bfba-4e8f-86af-28f16bf0b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, LeaveOneGroupOut, ParameterGrid\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aaae67-7f3a-4d17-9dfb-b3f86bff4e24",
   "metadata": {},
   "source": [
    "### Step 1: Load and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ef73e2-4ef2-43c0-ac3d-bd9124cbc3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows and columns before removing NaNs: (36056, 109)\n",
      "Rows and columns after removing NaNs: (8368, 109)\n",
      "Features (X): (8368, 33), Target (y): (8368,), Groups: (8368,), Date: (8368,)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"C:/Data_MSc_Thesis/Pre_Processed_Data_Final/Pre_Processed_Data_All_Locations_V6.csv\"\n",
    "complete_dataset = pd.read_csv(data_path)\n",
    "\n",
    "column_to_remove = 'Porosity'  \n",
    "complete_dataset = complete_dataset.drop(columns=[column_to_remove])\n",
    "rename_dict = {\"Porosity_BIS4D_SOM\": \"Porosity\",\"BOFEK_2020_Physical Units\": \"BOFEK_PU\", \"Peat_Thickness_2022\": \"Peat_Thickness\"}\n",
    "complete_dataset.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# Data preprocessing\n",
    "complete_dataset['Date'] = pd.to_datetime(complete_dataset['Date'], format='%Y-%m-%d')\n",
    "complete_dataset['Source_ID'] = complete_dataset['Site_ID'].astype('category').cat.codes + 1\n",
    "print(f\"Rows and columns before removing NaNs: {complete_dataset.shape}\")\n",
    "filtered_df = complete_dataset.dropna(subset=['SENTEK_SMC']).copy()\n",
    "filtered_df['BOFEK_PU'] = filtered_df['BOFEK_PU'].astype('category')\n",
    "print(f\"Rows and columns after removing NaNs: {filtered_df.shape}\")\n",
    "\n",
    "# All 34 features\n",
    "Features = ['Sentinel_1_SMC', 'S1_Backscatter', 'S1_Backscatter_SD', 'S2_NDVI', 'S2_EVI', 'S2_NDMI','L8_9_LST', 'MODIS_LAI', \n",
    "            'STMP_1_015', 'ATMP_f', 'PAIR_f', 'WTMP_f', 'WLEV_f', 'WIND_f', 'WINS_f', 'RHUM_f', 'RAIN_f', 'VPD_f', 'ET', 'PET', 'ET0', \n",
    "            'RZSM_OWASIS', 'GWL_OWASIS', 'AWS_OWASIS', 'BD_5_15', 'Clay_5_15', 'Sand_5_15', 'Silt_5_15', 'SOM_2023_5_15', 'Porosity', 'BOFEK_PU', 'Peat_Thickness','DOY']\n",
    "\n",
    "X = filtered_df[Features]         # Features   \n",
    "y = filtered_df['SENTEK_SMC']     # Predictor\n",
    "groups = filtered_df[\"Source_ID\"] # Groups for Leave-One-Group-Out\n",
    "dates = filtered_df['Date']\n",
    "print(f\"Features (X): {X.shape}, Target (y): {y.shape}, Groups: {groups.shape}, Date: {dates.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3ee16c-de39-485e-8ebb-3a4b19748d47",
   "metadata": {},
   "source": [
    "### Step 2: Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67eeb1b7-23b6-40fc-b88d-9149b009625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the train-test split with temporal separation\n",
    "X_train, X_test, y_train, y_test, groups_train, groups_test, dates_train, dates_test = train_test_split(\n",
    "    X, y, groups, dates,\n",
    "    test_size=0.1,      # Reserve 10% for the test set\n",
    "    shuffle=False)      # Ensure temporal order is maintained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1dbb21-cb56-44b1-b555-6036873e774b",
   "metadata": {},
   "source": [
    "#### Perform a couple of checks regarding the data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab504ac-5b37-44bc-9f0a-3091c08f4641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Earliest date in training set: 2021-09-01 00:00:00\n",
      "Latest date in training set: 2023-12-03 00:00:00\n",
      "Earliest date in test set: 2023-12-04 00:00:00\n",
      "Latest date in test set: 2024-05-13 00:00:00\n",
      "Data shapes after splitting and alignment:\n",
      "Train set: X_train: (7531, 33), y_train: (7531,), groups_train: (7531,), dates_train: (7531,)\n",
      "Test set: X_test: (837, 33), y_test: (837,), groups_test: (837,), dates_test: (837,)\n",
      "Group distribution in training set:\n",
      "Source_ID\n",
      "11    824\n",
      "12    823\n",
      "17    820\n",
      "7     804\n",
      "4     778\n",
      "8     773\n",
      "16    735\n",
      "2     716\n",
      "1     698\n",
      "3     560\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Group distribution in test set:\n",
      "Source_ID\n",
      "1     162\n",
      "12    162\n",
      "11    149\n",
      "16    142\n",
      "17    140\n",
      "2      82\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique groups in train: 10, in test: 6\n"
     ]
    }
   ],
   "source": [
    "# Verify temporal separation\n",
    "print(\"\\nEarliest date in training set:\", dates_train.min())\n",
    "print(\"Latest date in training set:\", dates_train.max())\n",
    "print(\"Earliest date in test set:\", dates_test.min())\n",
    "print(\"Latest date in test set:\", dates_test.max())\n",
    "\n",
    "# Ensure alignment of training data\n",
    "X_train, y_train = X_train.align(y_train, join='inner', axis=0)\n",
    "groups_train = groups_train.loc[X_train.index]\n",
    "dates_train = dates_train.loc[X_train.index]  # Align dates_train with X_train\n",
    "\n",
    "# Ensure alignment of test data\n",
    "X_test, y_test = X_test.align(y_test, join='inner', axis=0)\n",
    "groups_test = groups_test.loc[X_test.index]\n",
    "dates_test = dates_test.loc[X_test.index]  # Align dates_test with X_test\n",
    "\n",
    "# Verify alignment\n",
    "assert X_train.index.equals(y_train.index) and X_train.index.equals(groups_train.index) and X_train.index.equals(dates_train.index), \\\n",
    "    \"Rows in X_train, y_train, groups_train, and dates_train are misaligned!\"\n",
    "\n",
    "assert X_test.index.equals(y_test.index) and X_test.index.equals(groups_test.index) and X_test.index.equals(dates_test.index), \\\n",
    "    \"Rows in X_test, y_test, groups_test, and dates_test are misaligned!\"\n",
    "\n",
    "# Print shapes for verification\n",
    "print(\"Data shapes after splitting and alignment:\")\n",
    "print(f\"Train set: X_train: {X_train.shape}, y_train: {y_train.shape}, groups_train: {groups_train.shape}, dates_train: {dates_train.shape}\")\n",
    "print(f\"Test set: X_test: {X_test.shape}, y_test: {y_test.shape}, groups_test: {groups_test.shape}, dates_test: {dates_test.shape}\")\n",
    "print(\"Group distribution in training set:\")\n",
    "print(groups_train.value_counts())\n",
    "print(\"\\nGroup distribution in test set:\")\n",
    "print(groups_test.value_counts())\n",
    "\n",
    "# Calculate and print the number of unique groups in train and test sets\n",
    "unique_groups_train = groups_train.nunique()\n",
    "unique_groups_test = groups_test.nunique()\n",
    "print(f\"\\nUnique groups in train: {unique_groups_train}, in test: {unique_groups_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef764c33-3cdb-4f22-ae5f-bdac0681fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler to numerical features\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "X_train_scaled[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test_scaled[numeric_features] = scaler.transform(X_test[numeric_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847693a3-4481-4e9a-bd84-6ae06fc876c8",
   "metadata": {},
   "source": [
    "### Step 3: Define the model and hyperparameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f74562a9-7a61-4eac-9d86-40bdc122f07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 486 candidates, totalling 4860 fits\n"
     ]
    }
   ],
   "source": [
    "# Define monotonic constraints\n",
    "monotone_constraints = (\n",
    "    0,   # Sentinel_1_SMC\n",
    "    0,   # S1_Backscatter\n",
    "    0,   # S1_Backscatter_SD\n",
    "    0,   # S2_NDVI\n",
    "    0,   # S2_EVI\n",
    "    0,   # S2_NDMI\n",
    "   -1,   # L8_9_LST\n",
    "    0,   # MODIS_LAI\n",
    "   -1,   # STMP_1_015\n",
    "   -1,   # ATMP_f\n",
    "    0,   # PAIR_f\n",
    "   -1,   # WTMP_f\n",
    "    1,   # WLEV_f\n",
    "    0,   # WIND_f\n",
    "    0,   # WINS_f\n",
    "    0,   # RHUM_f \n",
    "    0,   # RAIN_f\n",
    "    0,   # VPD_f \n",
    "    0,   # ET \n",
    "    0,   # PET \n",
    "    0,   # ET0 \n",
    "    0,   # RZSM_OWASIS \n",
    "    0,   # GWL_OWASIS \n",
    "    0,   # AWS_OWASIS \n",
    "    0,   # BD_5_15 \n",
    "    0,   # Clay_5_15\n",
    "    0,   # Sand_5_15\n",
    "    0,   # Silt_5_15\n",
    "    0,   # SOM_2023_5_15 \n",
    "    0,   # Porosity\n",
    "    0,   # BOFEK_PU \n",
    "    0,   # Peat_Thickness\n",
    "    0    # DOY\n",
    ")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    \"xgbregressor__n_estimators\": [800, 900, 1000],\n",
    "    \"xgbregressor__max_depth\": [5, 6, 7],\n",
    "    \"xgbregressor__learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"xgbregressor__subsample\": [0.5, 0.7, 0.8],\n",
    "    \"xgbregressor__colsample_bytree\": [0.5, 0.6, 0.7],\n",
    "    \"xgbregressor__monotone_constraints\": [\n",
    "        monotone_constraints,\n",
    "        None\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define custom scoring functions\n",
    "def mae_scorer(y_true, y_pred):\n",
    "    return -mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def mse_scorer(y_true, y_pred):\n",
    "    return -mean_squared_error(y_true, y_pred)\n",
    "\n",
    "def bias_scorer(y_true, y_pred):\n",
    "    return np.mean(y_true - y_pred)\n",
    "\n",
    "scoring = {'r2': 'r2',\n",
    "           'mae': make_scorer(mae_scorer),\n",
    "           'mse': make_scorer(mse_scorer),\n",
    "           'bias': make_scorer(bias_scorer)}\n",
    "\n",
    "# Initialize model and pipeline\n",
    "xgb = XGBRegressor(random_state=42, enable_categorical=True)\n",
    "pipeline = Pipeline([(\"xgbregressor\", xgb)])\n",
    "\n",
    "# Set up cross-validation\n",
    "logo = LeaveOneGroupOut()\n",
    "warnings.simplefilter(\"error\", FitFailedWarning)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=logo,\n",
    "                           scoring=scoring, refit='r2', n_jobs=-1, error_score=0, verbose=3)\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train, groups=groups_train)\n",
    "\n",
    "# Extract results\n",
    "cv_results = grid_search.cv_results_\n",
    "results_list = []\n",
    "for i in range(len(cv_results['params'])):\n",
    "    params = cv_results['params'][i]\n",
    "    results_list.append({\n",
    "        'n_estimators': params['xgbregressor__n_estimators'],\n",
    "        'max_depth': params['xgbregressor__max_depth'],\n",
    "        'learning_rate': params['xgbregressor__learning_rate'],\n",
    "        'subsample': params['xgbregressor__subsample'],\n",
    "        'colsample_bytree': params['xgbregressor__colsample_bytree'],\n",
    "        'monotone_constraints': params.get('xgbregressor__monotone_constraints'),\n",
    "        'mean_test_r2': cv_results['mean_test_r2'][i],\n",
    "        'mean_test_mae': -cv_results['mean_test_mae'][i],\n",
    "        'mean_test_mse': -cv_results['mean_test_mse'][i],\n",
    "        'mean_test_bias': cv_results['mean_test_bias'][i]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e774ec-a8b6-41d7-85bb-36a97130786f",
   "metadata": {},
   "source": [
    "### Step 4: Print and save best hyperparameters and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a763aa29-69c2-4a48-8995-c886f4be6c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters: {'xgbregressor__colsample_bytree': 0.5, 'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 6, 'xgbregressor__monotone_constraints': None, 'xgbregressor__n_estimators': 800, 'xgbregressor__subsample': 0.5}\n",
      "Best Cross-Validation Score (R²): 0.4427785380474865\n"
     ]
    }
   ],
   "source": [
    "# Print best parameters and score\n",
    "print(\"\\nBest Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score (R²):\", grid_search.best_score_)\n",
    "\n",
    "# Save results to CSV for reference\n",
    "#results_df.to_csv(\"C:/Data_MSc_Thesis/Results/15_04_HP_1_grid_search_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b538027e-ece4-49ad-a039-db09f349ea5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
