{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4645ed2-0f7b-415e-88bf-39dc82f05dd3",
   "metadata": {},
   "source": [
    "**Script Description:** This script loads a pre-processed dataset, prepares the reduced selected set of features for predicting SENTEK Soil Moisture Content (SENTEK_SMC), and optimizes a Extreme Gradient Boosting regression model using grouped cross-validation.\n",
    "\n",
    "**File Name:** 02_05_Hyperparameter_Tuning_Run_2.ipynb\n",
    "\n",
    "**Date:** 2025\n",
    "\n",
    "**Created by:** Rob Alamgir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ff005c-fdf8-47ef-8bfa-d443f2a6f956",
   "metadata": {},
   "source": [
    "##### Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92aa91a-0be1-441e-b0ea-d0854e56e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, LeaveOneGroupOut, ParameterGrid\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#help(XGBRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aaae67-7f3a-4d17-9dfb-b3f86bff4e24",
   "metadata": {},
   "source": [
    "### Step 1: Load Data & prep the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ef73e2-4ef2-43c0-ac3d-bd9124cbc3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows and columns before removing NaNs: (36056, 109)\n",
      "Rows and columns after removing NaNs: (8368, 109)\n",
      "Features (X): (8368, 10), Target (y): (8368,), Groups: (8368,), Date: (8368,)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"C:/Data_MSc_Thesis/Pre_Processed_Data_Final/Pre_Processed_Data_All_Locations_V6.csv\"\n",
    "complete_dataset = pd.read_csv(data_path)\n",
    "\n",
    "column_to_remove = 'Porosity'  \n",
    "complete_dataset = complete_dataset.drop(columns=[column_to_remove])\n",
    "rename_dict = {\"Porosity_BIS4D_SOM\": \"Porosity\",\"BOFEK_2020_Physical Units\": \"BOFEK_PU\", \"Peat_Thickness_2022\": \"Peat_Thickness\"}\n",
    "complete_dataset.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# Data preprocessing\n",
    "complete_dataset['Date'] = pd.to_datetime(complete_dataset['Date'], format='%Y-%m-%d')\n",
    "complete_dataset['Source_ID'] = complete_dataset['Site_ID'].astype('category').cat.codes + 1\n",
    "print(f\"Rows and columns before removing NaNs: {complete_dataset.shape}\")\n",
    "filtered_df = complete_dataset.dropna(subset=['SENTEK_SMC']).copy()\n",
    "filtered_df['BOFEK_PU'] = filtered_df['BOFEK_PU'].astype('category')\n",
    "print(f\"Rows and columns after removing NaNs: {filtered_df.shape}\")\n",
    "\n",
    "# All selected 10 features\n",
    "Features = ['Sentinel_1_SMC', 'S1_Backscatter', 'S1_Backscatter_SD', \n",
    "            'S2_NDVI', 'S2_NDMI', 'L8_9_LST', 'WLEV_f', 'WTMP_f', 'PET', 'Peat_Thickness']\n",
    "\n",
    "X = filtered_df[Features]         # Features   \n",
    "y = filtered_df['SENTEK_SMC']     # Predictor\n",
    "groups = filtered_df[\"Source_ID\"] # Groups for Leave-One-Group-Out\n",
    "dates = filtered_df['Date']\n",
    "print(f\"Features (X): {X.shape}, Target (y): {y.shape}, Groups: {groups.shape}, Date: {dates.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3ee16c-de39-485e-8ebb-3a4b19748d47",
   "metadata": {},
   "source": [
    "### Step 2: Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67eeb1b7-23b6-40fc-b88d-9149b009625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the train-test split with temporal separation\n",
    "X_train, X_test, y_train, y_test, groups_train, groups_test, dates_train, dates_test = train_test_split(\n",
    "    X, y, groups, dates,\n",
    "    test_size=0.1,      # Reserve 30% for the test set\n",
    "    shuffle=False)      # Ensure temporal order is maintained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1dbb21-cb56-44b1-b555-6036873e774b",
   "metadata": {},
   "source": [
    "#### Perform a couple of checks regarding the data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab504ac-5b37-44bc-9f0a-3091c08f4641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Earliest date in training set: 2021-09-01 00:00:00\n",
      "Latest date in training set: 2023-12-03 00:00:00\n",
      "Earliest date in test set: 2023-12-04 00:00:00\n",
      "Latest date in test set: 2024-05-13 00:00:00\n",
      "Data shapes after splitting and alignment:\n",
      "Train set: X_train: (7531, 10), y_train: (7531,), groups_train: (7531,), dates_train: (7531,)\n",
      "Test set: X_test: (837, 10), y_test: (837,), groups_test: (837,), dates_test: (837,)\n",
      "Group distribution in training set:\n",
      "Source_ID\n",
      "11    824\n",
      "12    823\n",
      "17    820\n",
      "7     804\n",
      "4     778\n",
      "8     773\n",
      "16    735\n",
      "2     716\n",
      "1     698\n",
      "3     560\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Group distribution in test set:\n",
      "Source_ID\n",
      "1     162\n",
      "12    162\n",
      "11    149\n",
      "16    142\n",
      "17    140\n",
      "2      82\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique groups in train: 10, in test: 6\n"
     ]
    }
   ],
   "source": [
    "# Verify temporal separation\n",
    "print(\"\\nEarliest date in training set:\", dates_train.min())\n",
    "print(\"Latest date in training set:\", dates_train.max())\n",
    "print(\"Earliest date in test set:\", dates_test.min())\n",
    "print(\"Latest date in test set:\", dates_test.max())\n",
    "\n",
    "# Ensure alignment of training data\n",
    "X_train, y_train = X_train.align(y_train, join='inner', axis=0)\n",
    "groups_train = groups_train.loc[X_train.index]\n",
    "dates_train = dates_train.loc[X_train.index]  # Align dates_train with X_train\n",
    "\n",
    "# Ensure alignment of test data\n",
    "X_test, y_test = X_test.align(y_test, join='inner', axis=0)\n",
    "groups_test = groups_test.loc[X_test.index]\n",
    "dates_test = dates_test.loc[X_test.index]  # Align dates_test with X_test\n",
    "\n",
    "# Verify alignment\n",
    "assert X_train.index.equals(y_train.index) and X_train.index.equals(groups_train.index) and X_train.index.equals(dates_train.index), \\\n",
    "    \"Rows in X_train, y_train, groups_train, and dates_train are misaligned!\"\n",
    "\n",
    "assert X_test.index.equals(y_test.index) and X_test.index.equals(groups_test.index) and X_test.index.equals(dates_test.index), \\\n",
    "    \"Rows in X_test, y_test, groups_test, and dates_test are misaligned!\"\n",
    "\n",
    "# Print shapes for verification\n",
    "print(\"Data shapes after splitting and alignment:\")\n",
    "print(f\"Train set: X_train: {X_train.shape}, y_train: {y_train.shape}, groups_train: {groups_train.shape}, dates_train: {dates_train.shape}\")\n",
    "print(f\"Test set: X_test: {X_test.shape}, y_test: {y_test.shape}, groups_test: {groups_test.shape}, dates_test: {dates_test.shape}\")\n",
    "\n",
    "# Print group distribution in train and test sets\n",
    "print(\"Group distribution in training set:\")\n",
    "print(groups_train.value_counts())\n",
    "print(\"\\nGroup distribution in test set:\")\n",
    "print(groups_test.value_counts())\n",
    "\n",
    "# Calculate and print the number of unique groups in train and test sets\n",
    "unique_groups_train = groups_train.nunique()\n",
    "unique_groups_test = groups_test.nunique()\n",
    "print(f\"\\nUnique groups in train: {unique_groups_train}, in test: {unique_groups_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59e4aa70-3bc2-465d-98ef-162d422777c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler to numerical features\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "X_train_scaled[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test_scaled[numeric_features] = scaler.transform(X_test[numeric_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847693a3-4481-4e9a-bd84-6ae06fc876c8",
   "metadata": {},
   "source": [
    "### Step 3: Define the model and hyperparameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca900df9-0d3e-41a0-8d1c-f320fb6ddeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 243 candidates, totalling 2430 fits\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"xgbregressor__n_estimators\": [800, 900, 1000],\n",
    "    \"xgbregressor__max_depth\": [5, 6, 7],\n",
    "    \"xgbregressor__learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"xgbregressor__subsample\": [0.5, 0.7, 0.8],\n",
    "    \"xgbregressor__colsample_bytree\": [0.5, 0.6, 0.7]\n",
    "}\n",
    "\n",
    "# Define custom scoring functions\n",
    "def mae_scorer(y_true, y_pred):\n",
    "    return -mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def mse_scorer(y_true, y_pred):\n",
    "    return -mean_squared_error(y_true, y_pred)\n",
    "\n",
    "def bias_scorer(y_true, y_pred):\n",
    "    return np.mean(y_true - y_pred)\n",
    "\n",
    "scoring = {'r2': 'r2',\n",
    "           'mae': make_scorer(mae_scorer),\n",
    "           'mse': make_scorer(mse_scorer),\n",
    "           'bias': make_scorer(bias_scorer)}\n",
    "\n",
    "# Initialize model and pipeline\n",
    "xgb = XGBRegressor(random_state=42, enable_categorical=True)\n",
    "pipeline = Pipeline([(\"xgbregressor\", xgb)])\n",
    "\n",
    "# Set up cross-validation\n",
    "logo = LeaveOneGroupOut()\n",
    "warnings.simplefilter(\"error\", FitFailedWarning)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=logo,\n",
    "                           scoring=scoring, refit='r2', n_jobs=-1, error_score=0, verbose=3)\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train, groups=groups_train)\n",
    "\n",
    "# Extract results\n",
    "cv_results = grid_search.cv_results_\n",
    "results_list = []\n",
    "for i in range(len(cv_results['params'])):\n",
    "    params = cv_results['params'][i]\n",
    "    results_list.append({\n",
    "        'n_estimators': params['xgbregressor__n_estimators'],\n",
    "        'max_depth': params['xgbregressor__max_depth'],\n",
    "        'learning_rate': params['xgbregressor__learning_rate'],\n",
    "        'subsample': params['xgbregressor__subsample'],\n",
    "        'colsample_bytree': params['xgbregressor__colsample_bytree'],\n",
    "        'mean_test_r2': cv_results['mean_test_r2'][i],\n",
    "        'mean_test_mae': -cv_results['mean_test_mae'][i],\n",
    "        'mean_test_mse': -cv_results['mean_test_mse'][i],\n",
    "        'mean_test_bias': cv_results['mean_test_bias'][i]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00365166-088f-4848-b0a4-35697eba7537",
   "metadata": {},
   "source": [
    "### Step 4: Print and save best hyperparameters and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c58cc381-9fb2-4882-98ba-9b83e26dff37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters: {'xgbregressor__colsample_bytree': 0.5, 'xgbregressor__learning_rate': 0.01, 'xgbregressor__max_depth': 5, 'xgbregressor__n_estimators': 800, 'xgbregressor__subsample': 0.8}\n",
      "Best Cross-Validation Score (R²): 0.42066701368232007\n"
     ]
    }
   ],
   "source": [
    "# Print best parameters and score\n",
    "print(\"\\nBest Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score (R²):\", grid_search.best_score_)\n",
    "\n",
    "# Save results to CSV for reference\n",
    "results_df.to_csv(\"C:/Data_MSc_Thesis/Results/04_05_HP_2_grid_search_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
